# -*- coding: utf-8 -*-
"""Data Extraction and NLP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16roQVwtWEyHkDU6RsVNg8vs-Je8dNEOV

# Data Collection
"""

import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import concurrent.futures

input_df = pd.read_excel('Input.xlsx')
input_df

def extract_data(url):
    try:
        response = requests.get(url)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")

        title = soup.find("title").get_text()

        divs = soup.find_all('div', class_=["td-post-content tagdiv-type", "tdb-block-inner td-fix-index"])

        article_text = ""
        for div in divs:
            paragraphs = div.find_all(["p", "li", "ul"])
            for element in paragraphs:
                if element.name == "ul":
                    list_items = element.find_all("li")
                    for li in list_items:
                        article_text += li.get_text()
                else:
                    article_text += element.get_text()

        return title, article_text.strip()

    except Exception as e:
        return None, None

def process_row(row):
    title, text = extract_data(row.URL)
    return title, text

import concurrent.futures

with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    results = list(executor.map(process_row, input_df.itertuples(index=False)))

input_df["Title"], input_df["Text"] = zip(*results)

input_df

"""#Data Preprocessing"""

input_df = input_df.drop([24, 37])
input_df = input_df.reset_index(drop=True)

input_df['Text'].isna().values

"""Lowercasing"""

input_df['Text'] = input_df['Text'].str.lower()

"""Remove HTML tags"""

# Function for Reomiving HTML Tags
def remove_html_tags(text):
  soup = BeautifulSoup(text, "html.parser")
  return soup.get_text()
  input_df['Text'] = input_df['Text'].apply(remove_html_tags)

"""Reomve Punctuations"""

import string

# Function for Reomiving Punctuations
def remove_punctuations(text):
    translator = str.maketrans('', '', string.punctuation + ':–“”')
    return text.translate(translator)

input_df['Text'] = input_df['Text'].apply(remove_punctuations)

"""# (1)Sentiment Analysis

(1.1) Cleaning using Stop Words Lists
"""

# Import Stop Words files
stopword_files = ['StopWords_DatesandNumbers.txt', 'StopWords_Geographic.txt', 'StopWords_Generic.txt', 'StopWords_Currencies.txt', 'StopWords_Auditor.txt', 'StopWords_Names.txt', 'StopWords_GenericLong.txt']

custom_stop_words = set()

# Function for Merging all Stop Words
def merge_stop_words_from_file(file_path):
    try:
        with open(file_path, 'r', encoding='ISO-8859-1', errors='ignore') as file:
            stop_words_in_file = file.read().split()
            custom_stop_words.update(stop_words_in_file)
    except Exception as e:
        print(f"Error reading {file_path}: {e}")

for file_path in stopword_files:
    merge_stop_words_from_file(file_path)

# Function for Reomiving Stop Words
def remove_stop_words(text):
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in custom_stop_words]
    return ' '.join(filtered_words)

# Apply "reomove_stop_words" function
input_df['Text'] = input_df['Text'].apply(remove_stop_words)

"""(1.2)	Creating a dictionary of Positive and Negative words"""

# Create a dictionary of Positive & Negative words if not not found stop words list

positive_words = {}
negative_words = {}

with open('positive-words.txt', 'r', encoding='utf-8') as file:
    positive_words_list = file.read().split()
    positive_words = {word: 1 for word in positive_words_list if word not in custom_stop_words}

with open('negative-words.txt', 'r', encoding='latin-1') as file:
    negative_words_list = file.read().split()
    negative_words = {word: -1 for word in negative_words_list if word not in custom_stop_words}

"""Tokenization"""

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

# Tokenize the Text
input_df['Tokens'] = input_df['Text'].apply(lambda text: word_tokenize(text))

"""Preprocess Output file"""

output_df = pd.read_excel('Output Data Structure.xlsx')
output_df = output_df.drop([24, 37])
output_df = output_df.reset_index(drop=True)
output_df

"""(1.3) Extracting Derived variables"""

# Function to calculate Positive Score
def calculate_positive_score(tokens, positive_words):
    return sum(1 for token in tokens if token in positive_words)

# Function to calculate Negative Score
def calculate_negative_score(tokens, negative_words):
    return sum(1 for token in tokens if token in negative_words)

# Function to calculate Polarity Score
def calculate_polarity_score(positive_score, negative_score):
    return (positive_score - negative_score) / (positive_score + negative_score + 0.000001)

# Function to calculate Subjectivity Score
def calculate_subjectivity_score(positive_score, negative_score, total_words):
    return (positive_score + negative_score) / (total_words + 0.000001)

# Calculate scores for each row in input_df
for index, row in input_df.iterrows():
    tokens = row['Tokens']
    total_words = len(tokens)

    positive_score = calculate_positive_score(tokens, positive_words)
    negative_score = calculate_negative_score(tokens, negative_words)
    polarity_score = calculate_polarity_score(positive_score, negative_score)
    subjectivity_score = calculate_subjectivity_score(positive_score, negative_score, total_words)

    # Store the calculated scores in the corresponding rows of output_df
    output_df.at[index, 'POSITIVE SCORE'] = positive_score
    output_df.at[index, 'NEGATIVE SCORE'] = negative_score
    output_df.at[index, 'POLARITY SCORE'] = polarity_score
    output_df.at[index, 'SUBJECTIVITY SCORE'] = subjectivity_score

output_df

"""# (2) Analysis of Readability"""

! pip install textstat

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from textstat import syllable_count

# Function to calculate Average Sentence Length
def calculate_average_sentence_length(text):
    sentences = sent_tokenize(text)
    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)
    total_sentences = len(sentences)
    return total_words / total_sentences

# Function to calculate Percentage of Complex Words
def calculate_percentage_complex_words(text):
    words = word_tokenize(text)
    complex_word_count = sum(1 for word in words if syllable_count(word) >= 3)
    total_word_count = len(words)
    return (complex_word_count / total_word_count) * 100

# Function to calculateFog Index
def calculate_gunning_fog_index(average_sentence_length, percentage_complex_words):
    return 0.4 * (average_sentence_length + percentage_complex_words)

# Calculate readability metrics for each row in input_df
for index, row in input_df.iterrows():
    text = row['Text']
    average_sentence_length = calculate_average_sentence_length(text)
    percentage_complex_words = calculate_percentage_complex_words(text)
    fog_index = calculate_gunning_fog_index(average_sentence_length, percentage_complex_words)

    # Store the calculated metrics in the corresponding columns of output_df
    output_df.at[index, 'AVG SENTENCE LENGTH'] = average_sentence_length
    output_df.at[index, 'PERCENTAGE OF COMPLEX WORDS'] = percentage_complex_words
    output_df.at[index, 'FOG INDEX'] = fog_index

output_df

"""# (3) Average Number of Words Per Sentence"""

# Function to calculate the average number of words per sentence
def calculate_average_words_per_sentence(tokenized_sentences):
    total_words = sum(len(word_tokenize(sentence)) for sentence in tokenized_sentences)
    total_sentences = len(tokenized_sentences)
    return total_words / total_sentences

# Calculate the average number of words per sentence for each row in input_df
for index, row in input_df.iterrows():
    tokenized_sentences = sent_tokenize(row['Text'])
    average_words_per_sentence = calculate_average_words_per_sentence(tokenized_sentences)

    # Store the calculated metric in the corresponding column of output_df
    output_df.at[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = average_words_per_sentence

output_df

"""# (4) Complex Word Count"""

from nltk.corpus import cmudict

# Function to count syllables in a word using the CMU Pronouncing Dictionary
def count_syllables(word, pronouncing_dict):
    if word.lower() in pronouncing_dict:
        return max([len(list(y for y in x if y[-1].isdigit())) for x in pronouncing_dict[word.lower()]])
    else:
        # If the word is not found in the dictionary, assume it has 1 syllable
        return 1

# Load the CMU Pronouncing Dictionary
nltk.download('cmudict')
pronouncing_dict = cmudict.dict()

# Function to calculate the complex word count
def calculate_complex_word_count(text):
    words = word_tokenize(text)
    complex_word_count = sum(1 for word in words if count_syllables(word, pronouncing_dict) > 2)
    return complex_word_count

# Calculate the complex word count for each row in input_df
for index, row in input_df.iterrows():
    text = row['Text']
    complex_word_count = calculate_complex_word_count(text)

    # Store the calculated metric in the corresponding column of output_df
    output_df.at[index, 'COMPLEX WORD COUNT'] = complex_word_count

output_df

input_df.columns

"""# (5) Word Count"""

from nltk.corpus import stopwords
nltk.download('stopwords')

# Function to calculate the word count after removing stopwords and punctuation
def calculate_word_count(tokenized_words):
    # Remove stopwords and punctuation
    stop_words = set(stopwords.words('english'))
    words = [word for word in tokenized_words if word not in stop_words and word not in string.punctuation]

    # Count the remaining words
    word_count = len(words)
    return word_count

# Calculate the word count for each row in input_df using the tokenized words
for index, row in input_df.iterrows():
    tokenized_words = row['Tokens']
    word_count = calculate_word_count(tokenized_words)

    # Store the calculated metric in the corresponding column of output_df
    output_df.at[index, 'WORD COUNT'] = word_count

output_df

"""# (6) Syllable Count Per Word"""

def count_syllables(word):
    # Convert the word to lowercase
    word = word.lower()

    # Remove trailing 'es' and 'ed' (common exceptions)
    if word.endswith('es'):
        word = word[:-2]
    elif word.endswith('ed'):
        word = word[:-2]

    # Count the number of vowels in the word
    vowels = "aeiouy"
    count = 0
    prev_char = ""

    for char in word:
        if char in vowels and prev_char not in vowels:
            count += 1
        prev_char = char

    # Handle words with no vowels
    if count == 0:
        count = 1

    return count

# Store the Syllable Count in the corresponding column of output_df
output_df['SYLLABLE PER WORD'] = input_df['Tokens'].apply(lambda tokens: [count_syllables(word) for word in tokens])

output_df

"""# (7) Personal Pronouns"""

import re

# Define a function to count personal pronouns
def count_personal_pronouns(text):
    # Define a regex pattern to match the specified personal pronouns
    pattern = r'\b(I|we|my|ours|us)\b'

    # Use re.findall to find all matches in the text
    matches = re.findall(pattern, text, flags=re.IGNORECASE)

    # Exclude "US" from the list of matches
    matches = [match for match in matches if match.lower() != "us"]

    # Return the count of personal pronouns
    return len(matches)

# Store the  count of Personal Pronouns in the corresponding column of output_df
output_df['PERSONAL PRONOUNS'] = input_df['Text'].apply(count_personal_pronouns)

output_df

"""# (8) Average Word Length"""

# Define a function to calculate average word length
def calculate_average_word_length(text):
    words = text.split()
    total_characters = sum(len(word) for word in words)
    total_words = len(words)

    if total_words == 0:
        return 0  # Avoid division by zero

    average_word_length = total_characters / total_words
    return average_word_length

# Store the calculated Avg Word Length in the corresponding column of output_df
output_df['AVG WORD LENGTH'] = input_df['Text'].apply(calculate_average_word_length)

output_df

"""# Save and Download the output file"""

# Save the output file

output_file_path = 'Output Data Structure.xlsx'
output_df.to_excel(output_file_path)

from google.colab import files
files.download(output_file_path)

